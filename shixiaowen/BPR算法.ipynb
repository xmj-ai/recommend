{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "# 贝叶斯个性化排序(Bayesian Persional Ranking BPR )算法原理及实战\n[TOC]\n---\n\n## 1、BPR算法简介\n## 1.1 基本思路\n\u003e在BPR算法中，我们将任意用户u对应的物品进行标记，如果用户u在同时有物品i和j的时候点击了i，那么我们就得到了一个三元组\u003cu,i,j\u003e，它表示对用户u来说，i的排序要比j靠前。如果对于用户u来说我们有m组这样的反馈，那么我们就可以得到m组用户u对应的训练样本。\n\n这里，我们做出两个假设：\n\n- 1.每个用户之间的偏好行为相互独立，即用户u在商品i和j之间的偏好和**其他用户无关**。\n- 2.同一用户对不同物品的偏序相互独立，也就是用户u在商品i和j之间的偏好和**其他的商品无关**。\n\n为了便于表述，我们用\u003eu符号表示用户u的偏好，上面的\u003cu,i,j\u003e可以表示为：i \u003eu j。\n\n在BPR中，我们也用到了类似矩阵分解的思想，对于用户集U和物品集I对应的U*I的预测排序矩阵，我们期望得到两个分解后的用户矩阵W(|U|×k)和物品矩阵H(|I|×k)，满足：\n\n$\\overline{X}\u003dWH^T$\n\n那么对于任意一个用户u，对应的任意一个物品i，我们预测得出的用户对该物品的偏好计算如下：\n\n$\\overline{x}_{ui}\u003dw_u \\times h_i\u003d\\displaystyle\\sum_{f\u003d1}^kw_{uf}h_{if}$\n\n\u003e而模型的最终目标是寻找合适的矩阵W和H，让X-(公式打不出来，这里代表的是X上面有一个横线，即W和H矩阵相乘后的结果)和X(实际的评分矩阵)最相似。看到这里，也许你会说，BPR和矩阵分解没有什区别呀？是的，到目前为止的基本思想是一致的，但是具体的算法运算思路，确实千差万别的，我们慢慢道来。\n\n\n## 1.2 算法运算思路\n\nBPR 基于最大后验估计P(W,H|\u003eu)来求解模型参数W,H,这里我们用θ来表示参数W和H, \u003eu代表用户u对应的所有商品的全序关系,则优化目标是P(θ|\u003eu)。根据贝叶斯公式，我们有：\n\n$P( \\theta |\u003eu)\u003d\\frac{P(\u003eu|\\theta)P(\\theta)}{P(\u003eu)}$\n\n由于我们求解假设了用户的排序和其他用户无关，那么对于任意一个用户u来说，P(\u003eu)对所有的物品一样，所以有：\n\n$P(\\theta|\u003eu)\\propto P(\u003eu|\\theta)P(\\theta)$\n\n这个优化目标转化为两部分。第一部分和样本数据集D有关，第二部分和样本数据集D无关。\n\n### 第一部分\n对于第一部分，由于我们假设每个用户之间的偏好行为相互独立，同一用户对不同物品的偏序相互独立，所以有：\n\n$\\displaystyle\\prod_{u \\in U} P(\u003eu|\\theta)\u003d\\displaystyle\\prod_{(u,i,j) \\in (U\\times I\\times I)} P(i\u003e_u j|\\theta)^{\\delta((u,i,j) \\in D)}(1-P(i\u003e_u j|\\theta))^{\\delta((u,i,j) \\notin D)}$\n\n其中，\n\n$\\delta(b)\u003d \\left\\{\\begin{matrix} 1 \u0026\\text{if b is true}\\\\ 0 \u0026else \\end{matrix}\\right.$\n\n上面的式子类似于极大似然估计，若用户u相比于j来说更偏向i，那么我们就希望P(i \u003eu j|θ)出现的概率越大越好。\n\n上面的式子可以进一步改写成：\n\n$\\displaystyle\\prod_{x \\in U} P(\u003eu|\\theta)\u003d \\prod_{(u,i,j) \\in D}P(i\u003e_u j|\\theta)$\n\n而对于P(i \u003eu j|θ)这个概率，我们可以使用下面这个式子来代替:\n\n$P(i\u003e_u j|\\theta)\u003d\\sigma (\\overline{x}_{uij}(\\theta))$\n\n其中，σ(x)是sigmoid函数，σ里面的项我们可以理解为用户u对i和j偏好程度的差异，我们当然希望i和j的差异越大越好，这种差异如何体现，最简单的就是差值：\n\n$\\overline{x}_{uij}(\\theta)\u003d\\overline{x}_{ui}(\\theta)-\\overline{x}_{uj}(\\theta)$\n\n因此优化目标的第一项可以写作：\n\n$\\displaystyle\\prod_{u \\in U} P(\u003e_u|\\theta)\u003d\\displaystyle\\prod_{(u,i,j) \\in D} \\sigma(\\overline{x}_{ui}-\\overline{x}_{uj})$\n\n哇，是不是很简单的思想，对于训练数据中的\u003cu,i,j\u003e，用户更偏好于i，那么我们当然希望在X-矩阵中ui对应的值比uj对应的值大，而且差距越大越好！\n\n### 第二部分\n回想之前我们通过贝叶斯角度解释正则化的文章：https://www.jianshu.com/p/4d562f2c06b8\n\n当θ的先验分布是正态分布时，其实就是给损失函数加入了正则项，因此我们可以假定θ的先验分布是正态分布：\n\n$P(\\theta) \\sim N(0,\\lambda_\\theta I)$ 所以：$\\ln P(\\theta)\u003d\\lambda||\\theta||^2$\n\n因此，最终的最大对数后验估计函数可以写作：\n\n$\\ln P(\\theta|\u003e_u) \\propto \\ln P(\u003e_u|\\theta)P(\\theta)\u003d\\ln \\displaystyle\\prod_{(u,i,j)\\in D} \\sigma(\\overline{x}_{ui}-\\overline{x}_{uj})+\\ln P(\\theta)\u003d\\displaystyle\\sum_{(u,i,j) \\in D} \\ln \\sigma(\\overline{x}_{ui}-\\overline{x}_{uj})+\\lambda||\\theta||^2$\n\n剩下的我们就可以通过梯度上升法(因为是要让上式最大化)来求解了。我们这里就略过了，BPR的思想已经很明白了吧，哈哈！让我们来看一看如何实现吧。\n\n## 2、算法实现\n本文的github地址为：https://github.com/princewen/tensorflow_practice/tree/master/recommendation/Basic-BPR-Demo\n\n所用到的数据集是movieslen 100k的数据集，下载地址为：http://grouplens.org/datasets/movielens/\n### 数据预处理\n\n首先，我们需要处理一下数据，得到每个用户打分过的电影，同时，还需要得到用户的数量和电影的数量。\n\n    def load_data():\n        user_ratings \u003d defaultdict(set)\n        max_u_id \u003d -1\n        max_i_id \u003d -1\n        with open(\u0027data/u.data\u0027,\u0027r\u0027) as f:\n            for line in f.readlines():\n                u,i,_,_ \u003d line.split(\"\\t\")\n                u \u003d int(u)\n                i \u003d int(i)\n                user_ratings[u].add(i)\n                max_u_id \u003d max(u,max_u_id)\n                max_i_id \u003d max(i,max_i_id)\n    \n    \n        print(\"max_u_id:\",max_u_id)\n        print(\"max_i_idL\",max_i_id)\n    \n        return max_u_id,max_i_id,user_ratings\n\n下面我们会对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，后面构造训练集和测试集需要用到。\n\n    def generate_test(user_ratings):\n        \"\"\"\n        对每一个用户u，在user_ratings中随机找到他评分过的一部电影i,保存在user_ratings_test，我们为每个用户取出的这一个电影，是不会在训练集中训练到的，作为测试集用。\n        \"\"\"\n        user_test \u003d dict()\n        for u,i_list in user_ratings.items():\n            user_test[u] \u003d random.sample(user_ratings[u],1)[0]\n        return user_test\n        \n### 构建训练数据\n我们构造的训练数据是\u003cu,i,j\u003e的三元组，i可以根据刚才生成的用户评分字典得到，j可以利用负采样的思想，认为用户没有看过的电影都是负样本：\n\n    def generate_train_batch(user_ratings,user_ratings_test,item_count,batch_size\u003d512):\n        \"\"\"\n        构造训练用的三元组\n        对于随机抽出的用户u，i可以从user_ratings随机抽出，而j也是从总的电影集中随机抽出，当然j必须保证(u,j)不在user_ratings中\n    \n        \"\"\"\n        t \u003d []\n        for b in range(batch_size):\n            u \u003d random.sample(user_ratings.keys(),1)[0]\n            i \u003d random.sample(user_ratings[u],1)[0]\n            while i\u003d\u003duser_ratings_test[u]:\n                i \u003d random.sample(user_ratings[u],1)[0]\n    \n            j \u003d random.randint(1,item_count)\n            while j in user_ratings[u]:\n                j \u003d random.randint(1,item_count)\n    \n            t.append([u,i,j])\n    \n        return np.asarray(t)\n   \n### 构造测试数据\n同样构造三元组，我们刚才给每个用户单独抽出了一部电影，这个电影作为i，而用户所有没有评分过的电影都是负样本j：\n\n    def generate_test_batch(user_ratings,user_ratings_test,item_count):\n        \"\"\"\n        对于每个用户u，它的评分电影i是我们在user_ratings_test中随机抽取的，它的j是用户u所有没有评分过的电影集合，\n        比如用户u有1000部电影没有评分，那么这里该用户的测试集样本就有1000个\n        \"\"\"\n        for u in user_ratings.keys():\n            t \u003d []\n            i \u003d user_ratings_test[u]\n            for j in range(1,item_count + 1):\n                if not(j in user_ratings[u]):\n                    t.append([u,i,j])\n            yield np.asarray(t)\n### 模型构建\n首先回忆一下我们需要学习的参数θ，其实就是用户矩阵W(|U|×k)和物品矩阵H(|I|×k)对应的值，对于我们的模型来说，可以简单理解为由id到embedding的转化，因此有：\n\n    u \u003d tf.placeholder(tf.int32,[None])\n    i \u003d tf.placeholder(tf.int32,[None])\n    j \u003d tf.placeholder(tf.int32,[None])\n    \n    user_emb_w \u003d tf.get_variable(\"user_emb_w\", [user_count + 1, hidden_dim],\n                                 initializer\u003dtf.random_normal_initializer(0, 0.1))\n    item_emb_w \u003d tf.get_variable(\"item_emb_w\", [item_count + 1, hidden_dim],\n                                 initializer\u003dtf.random_normal_initializer(0, 0.1))\n    \n    u_emb \u003d tf.nn.embedding_lookup(user_emb_w, u)\n    i_emb \u003d tf.nn.embedding_lookup(item_emb_w, i)\n    j_emb \u003d tf.nn.embedding_lookup(item_emb_w, j)\n\n回想一下我们要优化的目标，第一部分是ui和uj对应的预测值的评分之差，再经由sigmoid变换得到的[0,1]值，我们希望这个值越大越好，对于损失来说，当然是越小越好。因此，计算如下：\n\n    x \u003d tf.reduce_sum(tf.multiply(u_emb,(i_emb-j_emb)),1,keep_dims\u003dTrue)\n    loss1 \u003d - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n\n第二部分是我们的正则项，参数就是我们的embedding值，所以正则项计算如下：\n\n    l2_norm \u003d tf.add_n([\n            tf.reduce_sum(tf.multiply(u_emb, u_emb)),\n            tf.reduce_sum(tf.multiply(i_emb, i_emb)),\n            tf.reduce_sum(tf.multiply(j_emb, j_emb))\n        ])\n因此，我们模型整个的优化目标可以写作：\n\n    regulation_rate \u003d 0.0001\n    bprloss \u003d regulation_rate * l2_norm - tf.reduce_mean(tf.log(tf.sigmoid(x)))\n    \n    train_op \u003d tf.train.GradientDescentOptimizer(0.01).minimize(bprloss)\n## 3、总结\n\n- 1.BPR是基于矩阵分解的一种排序算法，它不是做全局的评分优化，而是针对每一个用户自己的商品喜好分贝做排序优化。\n- 2.它是一种pairwise的排序算法，对于每一个三元组\u003cu,i,j\u003e，模型希望能够使用户u对物品i和j的差异更明显。\n- 3.同时，引入了贝叶斯先验，假设参数服从正态分布，在转换后变为了L2正则，减小了模型的过拟合。\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "3\n3\n2\n2\n1\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import random\n\n\nfor i in range(5):\n    r\u003drandom.randint(1,4)\n    print(r)\n    \n\n参考链接：https://www.jianshu.com/p/ba1936ee0b69\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}